"""@package docstring
 XGboost.py Documentation
"""

# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sn4QTWRAbyljDlIGqqNvJswquul_B6Pl
"""


# from google.colab import drive
# drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# !ls
# %cd gdrive/My\ Drive/CS626/TextNormalization

import pandas as pd
import numpy as np
import os , re , gc , pickle
import xgboost as xgb
import numpy as np
import pandas as pd
# import keras
from sklearn.model_selection import train_test_split


labels = ["PLAIN", "PUNCT", "DATE", "LETTERS", "CARDINAL", "VERBATIM", "DECIMAL", "MEASURE", "MONEY", "ORDINAL", "TIME", "ELECTRONIC", "DIGIT", "FRACTION", "TELEPHONE", "ADDRESS"]


max_num_features = 10 ##tokens are padded/trancated to this length
pad_size = 1    ###no of next/previous tokens in feature vector
boundary_letter = -1    ###seperator
space_letter = 0      ###padding character
max_data_size = 10000  ###Trainingdata size

def context_window_transform(data, pad_size):     ###converts words sequence to feature set
    """ Converts token sequence to vector of feature vectors
        Feature vector of each preprocessed token is current token's encoding appended with encodings of previous and next tokens
        No of prev/next tokens to be included in feature vector for context in given by pad_size
    """
    pre = np.zeros(max_num_features)
    pre = [pre for x in np.arange(pad_size)]
    data = pre + data + pre
    neo_data = []
    for i in np.arange(len(data) - pad_size * 2):
        row = []
        for x in data[i : i + pad_size * 2 + 1]:
            row.append([boundary_letter])
            row.append(x)
        row.append([boundary_letter])
        neo_data.append([int(x) for y in row for x in y])
    return neo_data



model = None
def train_model():
    """ Trains a model on date of size max_data_size and saves the model
        No of training rounds is 10 by default can be changed in line 117
    """
    out_path = r'.'
    df = pd.read_csv(r'en_train.csv')

    y_data =  pd.factorize(df['class'])
    labels = y_data[1]
    y_data = y_data[0]

    x_data = []
    df = df.head(max_data_size)
    for x in df['before'].values:
        x_row = np.ones(max_num_features, dtype=int) * space_letter
        for xi, i in zip(list(str(x)), np.arange(max_num_features)):
            x_row[i] = ord(xi)
        x_data.append(x_row)

    x_data = x_data[:max_data_size]
    y_data = y_data[:max_data_size]
    x_data = np.array(context_window_transform(x_data, pad_size))
    gc.collect()
    x_data = np.array(x_data)
    y_data = np.array(y_data)

    print('Total number of samples:', len(x_data))
    print('Use: ', max_data_size)

    print('x_data sample:')
    print(x_data[0])
    print('y_data sample:')
    print(y_data[0])
    print('labels:')
    print(labels)

    x_train = x_data
    y_train = y_data
    gc.collect()

    x_train, x_valid, y_train, y_valid= train_test_split(x_train, y_train,test_size=0.8,shuffle=False)
    gc.collect()
    num_class = len(labels)
    dtrain = xgb.DMatrix(x_train, label=y_train)
    dvalid = xgb.DMatrix(x_valid, label=y_valid)
    watchlist = [(dvalid, 'valid'), (dtrain, 'train')]

    param = {'objective':'multi:softmax',
            'eta':'0.3', 'max_depth':10,
            'silent':1, 'nthread':-1,
            'num_class':num_class,
            'eval_metric':'merror'}
    
    global model
    model = xgb.train(param, dtrain, 10, watchlist, early_stopping_rounds=5,verbose_eval=1)
    model.save_model(os.path.join(out_path, 'xgb_model_{}'.format(max_data_size)))

    pred = model.predict(dvalid)
    pred = [labels[int(x)] for x in pred]
    y_valid = [labels[x] for x in y_valid]
    x_valid = [ [ chr(x) for x in y[2 + max_num_features: 2 + max_num_features * 2]] for y in x_valid]
    x_valid = [''.join(x) for x in x_valid]
    x_valid = [re.sub('a+$', '', x) for x in x_valid]
    gc.collect()

    accuracy = sum(a == b for a,b in zip(pred,y_valid))/len(pred)
    print("Accuracy :", accuracy)


def loadModel():
    """Loads previously trained Model xgb_model_3200000 by default
        Can be changed by changing next lines
    """
    global model
    model = xgb.Booster()  # init model
    model.load_model('xgb_model_3200000')  # load data 


# train_model()    #uncomment This Line To Train and Save Model and comment the next line to use this model

loadModel()        ##loadModel  


# !pip install singleton_decorator
###Rule Based Convertors
from converters.Plain      import Plain
from converters.Punct      import Punct
from converters.Date       import Date
from converters.Letters    import Letters
from converters.Cardinal   import Cardinal
from converters.Verbatim   import Verbatim
from converters.Decimal    import Decimal
from converters.Measure    import Measure
from converters.Money      import Money
from converters.Ordinal    import Ordinal
from converters.Time       import Time
from converters.Electronic import Electronic
from converters.Digit      import Digit
from converters.Fraction   import Fraction
from converters.Telephone  import Telephone
from converters.Address    import Address

labels = ["PLAIN", "PUNCT", "DATE", "LETTERS", "CARDINAL", "VERBATIM", "DECIMAL", "MEASURE", "MONEY", "ORDINAL", "TIME", "ELECTRONIC", "DIGIT", "FRACTION", "TELEPHONE", "ADDRESS"]
label_dict = {
    "PLAIN": Plain(),
    "PUNCT": Punct(),
    "DATE": Date(),
    "LETTERS": Letters(),
    "CARDINAL": Cardinal(),
    "VERBATIM": Verbatim(),
    "DECIMAL": Decimal(),
    "MEASURE": Measure(),
    "MONEY": Money(),
    "ORDINAL": Ordinal(),
    "TIME": Time(),
    "ELECTRONIC": Electronic(),
    "DIGIT": Digit(),
    "FRACTION": Fraction(),
    "TELEPHONE": Telephone(),
    "ADDRESS": Address(),
}
###seperate rule based converter for each semiotic class


def Normalise(sentence): 
    """ Normalises the sequence of tokes provided as string with '###' as delimiter and prints normalised sentence
    """
    l = sentence.split("###")
    x_data = []
    for x in l:
        x_row = np.ones(max_num_features, dtype=int) * space_letter
        for xi, i in zip(list(str(x)), np.arange(max_num_features)):
            x_row[i] = ord(xi)
        x_data.append(x_row)
    x_data = np.array(context_window_transform(x_data, pad_size))
    gc.collect()
    x_data = np.array(x_data)
    x_data = xgb.DMatrix(x_data)
    pred = model.predict(x_data)
    pred = [labels[int(x)] for x in pred]
    conv = [label_dict[pred[i]].convert(l[i]) for i in range(len(l))]
    pd.set_option('colheader_justify', 'center')
    sentdf = pd.DataFrame({'Before':l , 'Class Predicted':pred , "After":conv})
    print(sentdf)

sentence = "We###will###have###our###presentation##at###6:45PM###."
Normalise(sentence)

sentence = "The###total###cost###of###these###items###is###Rs300###."
Normalise(sentence)

sentence = """A###baby###giraffe###is###6 ft###tall###and###weighs###150 lb###."""
Normalise(sentence)

sentence = """call###me###at###999-8888-0011###."""
Normalise(sentence)